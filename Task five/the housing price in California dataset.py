# -*- coding: utf-8 -*-
"""Copy of Task_5_exercises.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SIjuFrBsg7EGqsAjzrijgg6Ug5591NGy

![](logo1.jpg)

# shAI Training 2022 | Level 1
## Task #5  (End-to-End ML Project {part_1})
***

### Welcome to the exercises for reviewing first part of end to end ML project.

**Make sure that you read and understand ch2 from the hands-on ML book (page 35 - 71) before start with this notebook.**

**If you stuck with anything reread that part from the book and feel free to ask about anything in the messenger group as you go along.**

**Good Luck : )**

***
# 1- Get the Data
***
"""

import numpy as np
import pandas as pd

"""#### We'll work with the housing price in California dataset
#### There're 2 ways to get the data:

## First:
Download and extract the dataset in your local device from

https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz

then read it using pandas method read_csv

**Read housing.csv as a dataframe called housing.**
"""

#housing = pd.read_csv('housing.csv')

"""## Second:
Write a simple function that gets the dataset directly from the website.
"""

# import needed libraries
import os
import tarfile
import urllib
import zipfile

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()

"""**Use load_housing_data to create dataframe called housing.**"""

def load_housing_data(housing_path=HOUSING_PATH):
    fetch_housing_data()
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

housing1 = load_housing_data(HOUSING_PATH)

"""***
# 2- Discover and visualize the data
***

## A- Data discovery

**Check the head of housing, and check out its info() and describe() methods.**

#### 1-Let’s take a look at the top five rows using the DataFrame’s head() method
"""

housing1.head()



"""#### 2-Use the info() method to get a quick description of the data"""

housing1.info()



"""#### 3-Let's take a look at how many districts belong to "ocean_proximity" by using the value_counts() method"""

housing1['ocean_proximity'].value_counts()



"""#### 4-Let’s look at the summary of the numerical attributes . Using the describe() method"""

housing1.describe()



"""## B- Data visualization

***NOTE: ALL THE COMMANDS FOR PLOTTING A FIGURE SHOULD ALL GO IN THE SAME CELL. SEPARATING THEM OUT INTO MULTIPLE CELLS MAY CAUSE NOTHING TO SHOW UP.***
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""#### Create a hist plot for housing dataframe as shown down"""

housing1.hist(bins=50, figsize=(20,15))



"""#### Create a scatter plot between "longitude" in x axis and "latitude" in y axis with alpha = 0.1"""

sns.scatterplot(x=housing1['longitude'],y=housing1['latitude'],alpha=0.1)



"""#### Make The radius of each circle represent the district’s population (option s), and the color represents the price (option c)."""

housing1.plot(kind='scatter',x="longitude", y="latitude",s=housing1['population']/100,c='median_house_value',figsize=(8,7),label='population',cmap=plt.get_cmap('jet'), colorbar=True,
             sharex=False)



"""#### Explore correlation between all continuous numeric variables using .corr() method."""

numhouse = housing1.select_dtypes(include='float64')
numhouse.corr()



"""#### Use seaborn method to convert the correlation matrix to a heatmap plot
#### It's usually a better way to look for correlations among the features
"""

plt.figure(figsize = (10,8))
sns.heatmap(numhouse.corr(), cmap = 'viridis',annot=True)
plt.show()



"""#### Another way to check for correlation between attributes is to use the pandas scatter_matrix() function"""

from pandas.plotting import scatter_matrix
attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]
scatter_matrix(housing1[attributes],figsize=[10,8])
plt.show()



"""#### *CHALLENGE*
#### change the color of the plot based on ocean_proximity category
#### hint: use seaborn pairplot
"""

housing1['Ocean'] = housing1['ocean_proximity'].astype(str)

attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age", "Ocean"]
sns.pairplot(housing1[attributes],hue="Ocean")
plt.show()



"""#### Create a scatter plot between median_income and median_house_value"""

housing1.plot(kind='scatter',x='median_income',y='median_house_value',alpha=0.2)



"""***
# 3- Prepare the data
***

## A- Data Cleaning

#### Create a Series that displays the total count of missing values per column.
"""

housing1.isna().sum()

"""#### the total_bedrooms attribute has some missing values. You have three options:
1. Get rid of the corresponding districts.
2. Get rid of the whole attribute.
3. Set the values to some value (zero, the mean, the median, etc.).

You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna()
methods:

#### Use the third option and fill the total_bedrooms null values with the median
"""

housing1['total_bedrooms'].fillna(housing1['total_bedrooms'].mean(),inplace=True)

"""#### Check if there any zeros in dataframe."""

(housing1 == 0).sum(axis=0)

"""#### Great there aren't any zeros in the data.
#### Zeros may sometimes be missing values so we need to take a closer look at them

## B- Attribute Combinations

#### create new attribute called rooms_per_household between total_rooms and households
"""

housing1['rooms_per_household'] = housing1['total_rooms']/housing1['households']

"""#### create new attribute called bedrooms_per_room between total_bedrooms and total_rooms"""

housing1['bedrooms_per_room'] = housing1['total_bedrooms']/housing1['total_rooms']

"""#### create new attribute called population_per_household between population and households"""

housing1['population_per_household'] = housing1['population']/housing1['households']

housing1.info()

"""#### Now let's look for correlation again"""

numhouse2 = housing1.select_dtypes(include='float64')
corr_matrix = numhouse2.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)

"""#### now let's remove old features("total_bedrooms", "total_rooms", "population", "households"), use drop method
#### Note: make inplace parameter True to save changes
"""

housing1.drop(['total_bedrooms', 'total_rooms','population','households'], axis=1,inplace=True)

"""## C- Handling Text and Categorical Attributes

#### We have 5 classes in ocean_proximity.
#### To handle this categorical feature we create housing_cat that contain ocean_proximity.
"""

print(housing1["ocean_proximity"].unique())

housing_cat = housing1[["ocean_proximity"]]
housing_cat

"""#### Now use sklearn OneHotEncoder to fit and transform housing_cat."""

from sklearn.preprocessing import OneHotEncoder
Hot = OneHotEncoder()
housing_cat_encoded = Hot.fit_transform(housing_cat)

housing_cat_encoded.toarray()

"""## D- Feature Scaling

#### To scale numerical values we created housing_num that contain numerical values.
#### Now use sklearn StandardScaler to fit and transform housing_num.
"""

from sklearn.preprocessing import StandardScaler
housing_num = housing1.drop(["ocean_proximity","Ocean"], axis=1)

Sdrd = StandardScaler()
Sdrd.fit_transform(housing_num)



"""# Custom Transformers

####  Here is a small transformer class that adds the combined attributes wediscussed earlier:
"""

from sklearn.base import BaseEstimator, TransformerMixin

# get the right column indices: safer than hard-coding indices 3, 4, 5, 6
rooms_ix, bedrooms_ix, population_ix, household_ix = [
    list(housing1.columns).index(col) for col in ("total_rooms", "total_bedrooms", "population", "households")]

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room = True):
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X, y=None):
        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
        population_per_household = X[:, population_ix] / X[:, household_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

"""#### Use the above class (CombinedAttributesAdder) to create instance called attr_reader, then transform housing values and save them in a variable called housing_extra_attribs."""

attr_reader = CombinedAttributesAdder()
housing_extra_attribs = attr_reader.transform(housing1.values)

print(housing_extra_attribs)

"""# Transformation Pipelines

"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from sklearn.impute import SimpleImputer

"""#### Create your own pipline for numerical attributes. It should contain SimpleImputer, CombinedAttributesAdder, and StandardScaler. call it num_pipeline."""

num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('adder', CombinedAttributesAdder(numhouse)),
    ('scaler', StandardScaler())
])

"""#### now create a full pipeline called full_pipeline , use num_pipeline for numerical attributes  and OneHotEncoder for catigorcal attributes ."""

from sklearn.compose import ColumnTransformer
num_attribs = list(numhouse)
cat_attribs = ["ocean_proximity"]

cat_encoder = OneHotEncoder()
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_pipeline, housing1.select_dtypes(include=[np.number])),
        ('cat', cat_encoder, housing1.ocean_proximity)
    ]
)
full_pipeline = Pipeline([
    ('preprocessor', preprocessor)
])

"""**bold text**#### fit and transform full_pipeline with housing data then saved it in housing_prepared"""

housing_prepared = full_pipeline.fit_transform(housing1)

print(housing_prepared[:5])

"""# 4- Create a Test Set and Train Set

#### Use model_selection.train_test_split from sklearn to split the data into training and testing sets.
#### Note: Set random_state=42 to get the same result
"""

from sklearn.model_selection import train_test_split
target = housing1.median_house_value
housing1.drop(['median_house_value'], axis=1,inplace=True)
x_train,x_test,y_train,y_test = train_test_split(housing1,target, test_size=0.2, random_state=42)

"""# Great Job!
# #shAI_Club
"""